{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d90bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahdi Anvari 610700002\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b49fb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.5732294 ]\n",
      " [ 1.55516843]\n",
      " [-2.09398694]\n",
      " [-0.5697091 ]\n",
      " [-1.16876101]\n",
      " [ 0.84267817]\n",
      " [ 1.65243229]\n",
      " [ 1.20983559]]\n",
      "0.0014815896486944794\n",
      "0.9998992643324027\n"
     ]
    }
   ],
   "source": [
    "# question 1.a\n",
    "# It generates synthetic data for a linear regression problem : \n",
    "# a design matrix X, a coefficient vector Beta, and adds noise to simulate a response vector Y.\n",
    "\n",
    "def GenerateData(n, p,q):\n",
    "    X = np.random.normal(size=(n, p))\n",
    "    Beta = np.random.normal(size=(p, 1))\n",
    "    SelectedColumns = np.random.choice(p, q, replace=False)\n",
    "    SelectedX = X[:, SelectedColumns]\n",
    "    SelectedBeta = Beta[SelectedColumns]\n",
    "    NewBeta = np.zeros_like(Beta)\n",
    "    NewBeta[SelectedColumns] = SelectedBeta\n",
    "    Epsilon = np.random.normal(scale=0.05, size=(n, 1))\n",
    "    Y = X.dot(NewBeta) + Epsilon\n",
    "    return(Y,X,NewBeta)\n",
    "\n",
    "Data = GenerateData(100,10,8)\n",
    "y = Data[0]\n",
    "x = Data[1]\n",
    "Beta = Data[2]\n",
    "print(Beta)\n",
    "x_train = x[:80,]\n",
    "x_test = x[80:,]\n",
    "y_train = y[:80]\n",
    "y_test = y[80:]\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "print(mse)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8393200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.80277949]\n",
      " [-0.24605351]\n",
      " [-0.00296861]\n",
      " [ 0.02103424]\n",
      " [ 0.36499013]]\n"
     ]
    }
   ],
   "source": [
    "# question 1.b\n",
    "# It computes the coefficients of a linear regression model using the closed-form solution\n",
    "\n",
    "def ClosedForm(X, Y):\n",
    "    Beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "    return(Beta)\n",
    "\n",
    "print(ClosedForm(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a74931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.73727378]\n",
      " [-0.21001671]\n",
      " [-0.02304932]\n",
      " [ 0.03147785]\n",
      " [ 0.30811234]]\n"
     ]
    }
   ],
   "source": [
    "# question 1.c\n",
    "# It performs gradient descent optimization to estimate the coefficients of a linear regression model\n",
    "\n",
    "def GradientDescent(X,Y,Alpha=0.001,TH=0.0001,MaxIter=10000):\n",
    "    n = np.shape(X)[1]\n",
    "    p = np.shape(X)[1]\n",
    "    Beta = np.zeros((p, 1))\n",
    "    PrevCost = float('inf')\n",
    "    for _ in range(MaxIter):\n",
    "        PredY = X.dot(Beta)\n",
    "        Error = PredY - Y\n",
    "        Cost = np.sum(Error ** 2)/2\n",
    "        Gradient = X.T.dot(Error)\n",
    "        Beta -= Alpha * Gradient\n",
    "        if abs(Cost-PrevCost) < TH:\n",
    "            break\n",
    "        PrevCost = Cost\n",
    "    return(Beta)\n",
    "\n",
    "print(GradientDescent(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b56482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.81845799e+00]\n",
      " [-2.46773301e-01]\n",
      " [-1.01382127e-03]\n",
      " [ 2.55283818e-02]\n",
      " [ 3.69818868e-01]]\n"
     ]
    }
   ],
   "source": [
    "# question 1.d\n",
    "# It utilizes scikit-learn's LinearRegression model to estimate the coefficients of a linear regression model\n",
    "\n",
    "def SciKitLinearReg(X,Y):\n",
    "    n = np.shape(X)[1]\n",
    "    Model = LinearRegression()\n",
    "    Model.fit(X,Y)\n",
    "    return np.array(Model.coef_).reshape(n,1)\n",
    "\n",
    "print(SciKitLinearReg(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f01f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.21446822e+00]\n",
      " [-2.28894495e-01]\n",
      " [-8.59626072e-04]\n",
      " [ 2.47606844e-02]\n",
      " [ 3.58627606e-01]]\n"
     ]
    }
   ],
   "source": [
    "# question 1.e\n",
    "# It implements the LASSO regression algorithm\n",
    "# Within the function, both X and Y are normalized\n",
    "# In each iteration, it updates the coefficient estimates Beta by considering the L1 penalty term and the gradient of the cost function\n",
    "\n",
    "def NormalizeVector(vector):\n",
    "    mean = np.mean(vector)\n",
    "    normalized_vector = vector - mean\n",
    "    norm = np.linalg.norm(normalized_vector)\n",
    "    return normalized_vector / norm\n",
    "\n",
    "def NormalizeMatrix(matrix):\n",
    "    col_means = np.mean(matrix, axis=0)\n",
    "    normalized_matrix = matrix - col_means\n",
    "    col_norms = np.linalg.norm(normalized_matrix, axis=0)\n",
    "    return normalized_matrix / col_norms\n",
    "\n",
    "def LASSOreg(X,Y,Lambda=0.0001,TH=0.0001,MaxIter=1000):\n",
    "    X = NormalizeMatrix(X)\n",
    "    Y = NormalizeMatrix(Y)\n",
    "    p = np.shape(X)[1]\n",
    "    Beta = np.zeros((p, 1))\n",
    "    PrevCost = float('inf')\n",
    "    for _ in range(MaxIter):\n",
    "        for i in range(p):\n",
    "            PredY = X.dot(Beta)\n",
    "            XK = (X[:, i])\n",
    "            Error = Y - PredY + (XK*Beta[i]).reshape(10,1)\n",
    "            NewXK = (XK.T.reshape(10,1))\n",
    "            Gradient = NewXK.T.dot(Error)\n",
    "            if Gradient < -Lambda:\n",
    "                Beta[i] = (Gradient + Lambda)\n",
    "            elif Gradient > Lambda:\n",
    "                Beta[i] = (Gradient - Lambda)\n",
    "            else:\n",
    "                Beta[i] = 0\n",
    "        Cost = np.sum(Error ** 2) / 2\n",
    "        if abs(Cost-PrevCost) < TH:\n",
    "            break\n",
    "        PrevCost = Cost\n",
    "    return(Beta)\n",
    "\n",
    "print(LASSOreg(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10bfc5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.81229170e+00]\n",
      " [-2.44645290e-01]\n",
      " [-4.31411920e-04]\n",
      " [ 2.43587663e-02]\n",
      " [ 3.66703533e-01]]\n"
     ]
    }
   ],
   "source": [
    "# question 1.f\n",
    "# It utilizes scikit-learn's Lasso model to perform LASSO regression\n",
    "\n",
    "def SciKitLASSO(X,Y):\n",
    "    n = np.shape(X)[1]\n",
    "    Model = Lasso(alpha=0.001 , max_iter=1000)\n",
    "    Model.fit(X,Y)\n",
    "    return np.array(Model.coef_).reshape(n,1)\n",
    "\n",
    "print(SciKitLASSO(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6564df-f161-45bd-8b28-da093e1cae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1781344",
   "metadata": {},
   "source": [
    "# question 2.a\n",
    "\n",
    "![question 2.a](C:/Users/M/Desktop/2aa.jpg)\n",
    "![question 2.a](C:/Users/M/Desktop/2ab.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d8c86-90f5-4025-b86e-afe1eccf328d",
   "metadata": {},
   "source": [
    "# question 2.b\n",
    "\n",
    "![question 2.a](C:/Users/M/Desktop/2ba.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e56997-4d22-44f0-8cc6-89c0add308b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplementary data 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ce8ab06-67ae-499c-9631-bdea70ff67a3",
   "metadata": {},
   "source": [
    "You :\n",
    "please explain me the gradient descent algorithm and how to set it up \n",
    "ChatGPT :\n",
    "Certainly! Gradient descent is an optimization algorithm commonly used in machine learning and optimization problems to minimize a function by iteratively moving in the direction of steepest descent of the function. It's particularly useful for finding the minimum of a cost function in the context of training machine learning models.\n",
    "\n",
    "Here's a step-by-step explanation of the gradient descent algorithm:\n",
    "\n",
    "1-Initialization: Start by initializing the parameters (coefficients) of the model to some arbitrary values. This could be zeros or random values.\n",
    "\n",
    "2-Compute the Gradient: Compute the gradient of the cost function with respect to each parameter. The gradient points in the direction of the steepest increase of the function.\n",
    "\n",
    "3-Update Parameters: Adjust the parameters in the opposite direction of the gradient to minimize the cost function. This adjustment is made by subtracting a fraction of the gradient (scaled by a learning rate) from the current parameter values.\n",
    "\n",
    "4-Repeat: Repeat steps 2 and 3 until convergence criteria are met, such as reaching a maximum number of iterations or the change in the cost function becoming sufficiently small.\n",
    "\n",
    "5-Convergence: Check for convergence by evaluating if the change in the cost function or parameter values is below a predefined threshold.\n",
    "\n",
    "The key component of gradient descent is the learning rate, denoted by α (alpha). This parameter controls the size of the steps taken during optimization. If the learning rate is too small, convergence may be slow, while if it's too large, the algorithm may overshoot the minimum or even diverge.\n",
    "\n",
    "Here's how to set up gradient descent:\n",
    "\n",
    "1-Define the Cost Function: Start by defining the cost function you want to minimize. This could be the mean squared error for linear regression, the cross-entropy loss for logistic regression, or any other appropriate function for your problem.\n",
    "\n",
    "2-Compute the Gradient: Calculate the gradient of the cost function with respect to each parameter using calculus or automatic differentiation libraries if available (such as TensorFlow or PyTorch).\n",
    "\n",
    "3-Choose a Learning Rate: Select an appropriate learning rate (α). You may need to experiment with different values to find one that works well for your problem.\n",
    "\n",
    "4-Initialize Parameters: Initialize the parameters (coefficients) of the model.\n",
    "\n",
    "5-Update Parameters: Use the gradient computed in step 2 to update the parameters using the update rule: θ := θ - α * ∇J(θ), where θ represents the parameters, α is the learning rate, and ∇J(θ) is the gradient of the cost function.\n",
    "\n",
    "Repeat Until Convergence: Repeat steps 2-5 until the algorithm converges to a minimum, as determined by a convergence criterion.\n",
    "\n",
    "Gradient descent is a fundamental optimization algorithm used in many machine learning algorithms, including linear regression, logistic regression, neural networks, and more. Understanding gradient descent and its variants is crucial for effectively training machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074dbbf-edf6-48b1-844a-5d024fae22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplementary data 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "993e9c6b-9ed7-4d6d-bdfe-fdd01c790bb5",
   "metadata": {},
   "source": [
    "You :\n",
    "just explain the mathematics of the coordinate descent algorithm for LASSO regression\n",
    "ChatGPT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1527c0-f539-4f13-8cdc-3fba9d2ab886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b990fc17-ef33-47e7-a2d7-e277894fa7f7",
   "metadata": {},
   "source": [
    "![](C:/Users/M/Desktop/Screenshot(3510).png)\n",
    "![](C:/Users/M/Desktop/Screenshot(3511).png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
